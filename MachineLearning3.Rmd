---
title: "Practical Machine Learning Project"
author: "Michael Shanley"
date: "October 24, 2015"
output: html_document
---

PRACTICAL MACHINE LEARNING COURSE PROJECT

SYNOPSIS

Training and test data from the following study:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

This assignment is to predict if the excercise was performed properly.  The second part of the model is to predict 20 different test cases.

Load Libriaries
Caret, Lattice, ggplot2, rpart, rpart.plot, RColorBrewer, rattle and random Forest libraries have been used.

```{r}
## Loading: caret
library(caret)
## Loading: rpart
library(rpart)
library(rpart.plot)
## Loading: RColorBrewer
library(RColorBrewer)
## Loading: rattle
library(rattle)
## Loading randomForest
library(randomForest)
```

INPUT DATA

Import the data then verify and validate.

set.seed(12345)
Getting the data
The training data set can be found on the following URL:
```{r}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
```
The testing data set can be found on the following URL:

```{r}
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

Put the data in memory
```{r}
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

Partioning Training data set into training and testing, 60% for myTraining, 40% for myTesting:
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

Cleaning the data

Cleaning NearZeroVariance Variables Run this code to view possible NZV Variables:
```{r}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
```
Run this code to create another subset without NZV variables:

```{r}
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
```
To check the number of observations
```{r}
dim(myTraining)
```

Removes the first column, we renamed:

```{r}
myTraining <- myTraining[c(-1)]
```
Gets rid of variables with 60% null values. If they are null, no need to deal with them.

```{r}
trainingV3 <- myTraining #creating subset to iterate in loop
for(i in 1:length(myTraining)) { #loop for each column
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if nulls > 60% 
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if columns are the same:
                trainingV3 <- trainingV3[ , -j] #Remove that column
            }   
        } 
    }
}
```
Check point
```{r}
dim(trainingV3)

#Seting back to our set:
myTraining <- trainingV3
rm(trainingV3)
```
Use testing data sets

```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) 
myTesting <- myTesting[clean1]
testing <- testing[clean2]

dim(myTesting)

dim(testing)


```
Make the data the same
```{r}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
```

```{r}
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
```
#Decision Tree:
```{r}
fancyRpartPlot(modFitA1)
```

```{r}
modFitB1 <- randomForest(classe ~. , data=myTraining)
```
#Predicting Error:
```{r}
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
```

# Statistics
                                         
#Accuracy : 0.999         
#95% CI : (0.997, 0.999)
#No Information Rate : 0.284         
#P-Value [Acc > NIR] : <2e-16        
                                         
                   Kappa : 0.998         
# Statistics:
 
                      Class: A Class: B Class: C Class: D Class: E
 Sensitivity             1.000    0.999    0.999    0.997    0.999
 Specificity             1.000    1.000    1.000    1.000    1.000
 Pos Pred Value          0.999    0.998    0.998    0.998    0.999
 Neg Pred Value          1.000    1.000    1.000    0.999    1.000
 Prevalence              0.284    0.193    0.174    0.164    0.184
 Detection Rate          0.284    0.193    0.174    0.163    0.184
 Detection Prevalence    0.285    0.194    0.174    0.164    0.184
 Balanced Accuracy       1.000    0.999    0.999    0.998    0.999


Random Forests created better results
